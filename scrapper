from apify_client import ApifyClient


client = ApifyClient("apify_api_HcROQbXE3Ci7qeZOMB6hXlRVBVs3KB0MwZ9D")

# Load URLs from the text file
urls = []
with open('reddit_urls.txt', 'r') as file:
    for line in file:
        urls.append(line.strip())

post_bodies = []


for url in urls:
    # Prepare the Actor input
    run_input = {
        "startUrls": [{ "url": url }],
        "maxItems": 100,
        "maxPostCount": 5,
        "maxComments": 100,
        "maxCommunitiesCount": 1,
        "maxUserCount": 20,
        "scrollTimeout": 40,
        "proxy": {
            "useApifyProxy": True,
            "apifyProxyGroups": ["RESIDENTIAL"],
        },
    }

  
    run = client.actor("trudax/reddit-scraper").call(run_input=run_input)

    print("ðŸ’¾ Check your data here: https://console.apify.com/storage/datasets/" + run["defaultDatasetId"])

    
    for item in client.dataset(run["defaultDatasetId"]).iterate_items():
        post_body = item.get("body")
        post_bodies.append(post_body)
    
  
    with open('scraped_data10.txt', 'a') as file:
        file.write(f"Scraped data for URL: {url}\n")
        for post_body in post_bodies:
            file.write("%s\n" % post_body)
        file.write("\n")  # Add a newline to separate data from different URLs
        post_bodies = []  # Reset post_bodies for the next URL

print("Scraped data saved to scraped_data.txt")


